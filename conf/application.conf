play.modules.enabled += "com.lifeway.it.order.fraud.GuiceInjectionModule"

play {
  http.secret.key = "GIkJCZiswl?4etf:ReawHcN3uSNqmd3HdeKlHZU32vzG<YEhggT[YG`8PJ[zz78`"
  i18n.langs = ["en"]

  application {
    loader = com.lifeway.it.order.fraud.FraudApplicationLoader
  }
}

//groupId, parallelism, batchSize, brokerList, topic.split(",").map(_.trim).toSeq)

fraud {
  messaging {
    subscribedTopics = "publicOrderProcessing"
    publishTopic = "CommerceShared"

    mongo {
      hosts = "localhost:27017"
      hosts = ${?MONGO_HOSTS}
      db = "fraudEvents"
      sslInvalidHostNameAllowed = "false"
    }

    kafka {
      groupId = "fraudService"
      parallelism = 9
      batchSize = 250
      brokers = "localhost:9092"
      brokers = ${?KAFKA_HOSTLIST}
    }
  }

  akka {
    #This and logback.xml should be changed to increase loglevel
    loglevel = INFO
    loggers = ["akka.event.slf4j.Slf4jLogger"]
    logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"

    serializers {
      java = "akka.serialization.JavaSerializer"
      kryo = "com.twitter.chill.akka.AkkaSerializer"
    }
    serialization-bindings {
    }

    kafka.consumer {
      kafka-clients {
        auto.offset.reset: "earliest"
        max.poll.interval.ms: 20000 // default 30000
        fetch.min.bytes: 0
        fetch.max.wait.ms: 500

        // Trying to avoid this error by reducing max poll records from the default...
        // org.apache.kafka.clients.consumer.CommitFailedException: Commit cannot be completed since the group has already
        // rebalanced and assigned the partitions to another member. This means that the time between subsequent calls to
        // poll() was longer than the configured max.poll.interval.ms, which typically implies that the poll loop is
        // spending too much time message processing. You can address this either by increasing the session timeout or by
        // reducing the maximum size of batches returned in poll() with max.poll.records.
        max.poll.records: 100 // default = 500 (old 2147483647)
      }

      # Tuning property of scheduled polls.
      poll-interval = 50ms

      # Tuning property of the `KafkaConsumer.poll` parameter.
      # Note that non-zero value means that blocking of the thread that
      # is executing the stage will be blocked.
      poll-timeout = 50ms

      # The stage will be await outstanding offset commit requests before
      # shutting down, but if that takes longer than this timeout it will
      # stop forcefully.
      stop-timeout = 30s

      # How long to wait for `KafkaConsumer.close`
      close-timeout = 20s

      # If offset commit requests are not completed within this timeout
      # the returned Future is completed `TimeoutException`.
      commit-timeout = 15s

      # If the KafkaConsumer can't connect to the broker the poll will be
      # aborted after this timeout. The KafkaConsumerActor will throw
      # org.apache.kafka.common.errors.WakeupException which will be ignored
      # until max-wakeups limit gets exceeded.
      wakeup-timeout = 6s

      # After exceeding maxinum wakeups the consumer will stop and the stage will fail.
      max-wakeups = 100

      # Fully qualified config path which holds the dispatcher configuration
      # to be used by the KafkaConsumerActor. Some blocking may occur.
      use-dispatcher = "akka.kafka.default-dispatcher"

      # Properties defined by org.apache.kafka.clients.consumer.ConsumerConfig
      # can be defined in this configuration section.
      kafka-clients {
        # Disable auto-commit by default
        enable.auto.commit = false
      }
    }
  }
}

cinnamon.chmetrics {
  reporters += "cinnamon.chmetrics.statsd-reporter"

  statsd-reporter {
    host = "127.0.0.1"
    port = 8125
  }

  actors {
    "/user/*" {
      report-by = class
    }
  }
}

config {
  // TODO: Change to false for non-local
  local = true
}
