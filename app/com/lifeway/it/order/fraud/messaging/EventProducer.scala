package com.lifeway.it.order.fraud.messaging

import java.util.concurrent.TimeUnit
import java.util.{Properties, UUID}

import kafka.message.{DefaultCompressionCodec, NoCompressionCodec}
import org.apache.kafka.clients.producer._
import org.apache.kafka.common.serialization.StringSerializer
import play.api.Logger

import scala.language.postfixOps
import scala.util.{Failure, Success, Try}

trait EventProducer {
  def send(message: String, key: String = null): Boolean

  def syncSend(message: String, key: String = null): Boolean

  def getTopic(): String
}

class DefaultKafkaProducer(
                              topic: String,

                              brokerList: String,

                              /**
                                * brokerList
                                * This is for bootstrapping and the producer will only use it for getting metadata (topics, partitions and replicas).
                                * The socket connections for sending the actual data will be established based on the broker information returned in
                                * the metadata. The format is host1:port1,host2:port2, and the list can be a subset of brokers or a VIP pointing to a
                                * subset of brokers.
                                */

                              clientId: String = UUID.randomUUID().toString,

                              /**
                                * clientId
                                * The client id is a user-specified string sent in each request to help trace calls. It should logically identify
                                * the application making the request.
                                */

                              synchronously: Boolean = true,

                              /**
                                * synchronously
                                * This parameter specifies whether the messages are sent asynchronously in a background thread.
                                * Valid values are false for asynchronous send and true for synchronous send. By setting the producer
                                * to async we allow batching together of requests (which is great for throughput) but open the possibility
                                * of a failure of the client machine dropping unsent data.
                                */

                              compress: Boolean = true,

                              /**
                                * compress
                                * This parameter allows you to specify the compression codec for all data generated by this producer.
                                * When set to true gzip is used.  To override and use snappy you need to implement that as the default
                                * codec for compression using SnappyCompressionCodec.codec instead of DefaultCompressionCodec.codec below.
                                */

                              batchSize: Integer = 200,

                              /**
                                * batchSize
                                * The number of messages to send in one batch when using async mode.
                                * The producer will wait until either this number of messages are ready
                                * to send or queue.buffer.max.ms is reached.
                                */

                              messageSendMaxRetries: Integer = 3,

                              /**
                                * messageSendMaxRetries
                                * This property will cause the producer to automatically retry a failed send request.
                                * This property specifies the number of retries when such failures occur. Note that
                                * setting a non-zero value here can lead to duplicates in the case of network errors
                                * that cause a message to be sent but the acknowledgement to be lost.
                                */

                              requestRequiredAcks: Integer = -1

                              /** requestRequiredAcks
                                * 0) which means that the producer never waits for an acknowledgement from the broker (the same behavior as 0.7).
                                * This option provides the lowest latency but the weakest durability guarantees (some data will be lost when a server fails).
                                * 1) which means that the producer gets an acknowledgement after the leader replica has received the data. This option provides
                                * better durability as the client waits until the server acknowledges the request as successful (only messages that were
                                * written to the now-dead leader but not yet replicated will be lost).
                                * -1) which means that the producer gets an acknowledgement after all in-sync replicas have received the data. This option
                                * provides the best durability, we guarantee that no messages will be lost as long as at least one in sync replica remains.
                                */) extends EventProducer {

  val props = new Properties()

  val codec = if (compress) DefaultCompressionCodec.codec else NoCompressionCodec.codec

  //  props.put("compression.codec", codec.toString)
  props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, brokerList)
  props.put(ProducerConfig.BATCH_SIZE_CONFIG, batchSize.toString)
  props.put(ProducerConfig.RETRIES_CONFIG, messageSendMaxRetries.toString)
  props.put(ProducerConfig.ACKS_CONFIG,requestRequiredAcks.toString)
  //props.put(ProducerConfig.PARTITIONER_CLASS_CONFIG, "kafka.producer.ByteArrayPartitioner")
  props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, classOf[StringSerializer].getName)
  props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, classOf[StringSerializer].getName)
  props.put("client.id", clientId.toString)

  val producer = new KafkaProducer[String, String](props)

  def syncSend(message: String, key: String = null): Boolean = {
    //val actualPartition = if (key == null) null else key.getBytes("UTF8")
    send(message, key)
  }

  def send(message: String, key: String = null): Boolean = {
    Try {
      producer.send(new ProducerRecord[String, String](topic, key, message)).get(5, TimeUnit.SECONDS)
      true
    } match {
      case Failure(e) =>
        Logger.error(e.getMessage)
        false
      case Success(_) => true
    }
  }

  def getTopic(): String = {
    topic
  }

  def close() {
    producer.close()
  }
}

object DefaultKafkaProducer {
  def apply(topic: String,
            brokerList: String,
            clientId: String = UUID.randomUUID().toString,
            synchronously: Boolean = true,
            compress: Boolean = true,
            batchSize: Integer = 200,
            messageSendMaxRetries: Integer = 0,
            requestRequiredAcks: Integer = 1): EventProducer = {
    new DefaultKafkaProducer(topic, brokerList, clientId, synchronously, compress, batchSize, messageSendMaxRetries, requestRequiredAcks)
  }
}